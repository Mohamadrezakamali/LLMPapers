# Large Language Models Papers

![](resources/image1.gif)

ðŸ”¥ Welcome to our dedicated GitHub repository, where we aim to share the most impactful papers and resources in the field of Artificial Intelligence (AI). As students deeply engrossed in the AI domain, our mission is to assemble a collection of pivotal and recent publications that are steering the future of this expansive field. We intend to explore a variety of areas within AI, starting with the game-changing capabilities of large language models (LLMs), which have transcended traditional language understanding to enable new forms of knowledge generation and reasoning.

Our repository not only emphasizes the advancements in LLMs but also delves into critical aspects such as explainability, privacy, security, and recommender systems. Explainability is key to making AI systems more interpretable and trustworthy, while privacy and security remain paramount in safeguarding data integrity. Recommender systems, a prime application of AI, illustrate the technology's vast influence on consumer behavior and societal trends. Through sharing selected papers, talks, tutorials, courses, and books, we aim to provide a comprehensive resource for anyone keen on grasping the current and future trajectories of AI, fostering a community that contributes to the field's growth and understanding.

---

## Table of Content

- [Large Language Models Papers](#Large-Language-Models-Papers)
  - [Papers](#papers)
    - [Milestone Papers](#Milestone-Papers)
    - [Explainability & Reasoning in AI](#Explainability-&-Reasoning-in-AI)
    - [Security & Privacy in AI](#Security-&-Privacy-in-AI)
    - [Recommender Systems](#Recommender-Systems)
    - [Other Notable Papers](#Other-Notable-Papers)
  - [Useful Resources](#Useful-Resources)
    - [Tutorials and Workshops](#Tutorials-and-Workshops)
    - [Comprehensive Courses](#Comprehensive-Courses)
    - [Inspirational Talks](#Inspirational-Talks)
    - [Essential Books](#Essential-Books)
    - [Other Resources](#Other-Resources)
  - [Contributing](#Contributing)



## Papers

### Milestone Papers

|  Date  |       keywords       |    Institute    | Paper                                                                                                                                                                               | Publication |
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: |
| 2017-06 |     Transformers     |      Google      | [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)                                                                                                                      |   NeurIPS<br>  ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount&query=%24.citationCount&label=citation) |
| 2018-10 |         BERT         |      Google      | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf)                                                              |    NAACL <br>![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf2b0e26d0599ce3e70df8a9da02e51594e0e992%3Ffields%3DcitationCount&query=%24.citationCount&label=citation)    |
| 2019-10 |          T5          |      Google      | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/v21/20-074.html)                                                           |    JMLR<br>  ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3cfb319689f06bf04c2e28399361f414ca32c4b3%3Ffields%3DcitationCount&query=%24.citationCount&label=citation)  |




### Explainability & Reasoning in AI 
1. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)
2. [Large Language Models are Zero-Shot Reasoners](https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Supplemental-Conference.pdf)
3. [Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)
4. [Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought](https://arxiv.org/abs/2210.01240)
5. [Reasoning in Large Language Models Through Symbolic Math Word Problems](https://arxiv.org/abs/2308.01906)
6. [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://arxiv.org/abs/2305.04388)

### Security & Privacy in AI

### Recommender Systems

### Other Notable Papers

## Useful Resources

### Tutorials and Workshops

### Comprehensive Courses

### Inspirational Talks

### Essential Books

### Other Resources

## Contributing

This structure aims to provide a clear and comprehensive guide to the most critical areas of AI research and application. By focusing on these key topics, we hope to facilitate a deeper understanding and appreciation of the advancements in artificial intelligence, fostering a community of learners, researchers, and enthusiasts eager to contribute to the field's growth and evolution.






